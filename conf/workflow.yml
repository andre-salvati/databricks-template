# The main job for default_python
resources:
  jobs:

    default_python_job:
      name: data_reporting_${bundle.target}
      timeout_seconds: 3600

      

      tasks:

        - task_key: extract_source1
          job_cluster_key: cluster-dev
          max_retries: 0
          python_wheel_task:
            package_name: template
            entry_point: main
            parameters: ["--task={{task.name}}",
                         "--env=${bundle.target}",
                         "${var.debug}"]
          libraries:
            - whl: ../dist/*.whl

        - task_key: extract_source2
          job_cluster_key: cluster-dev
          max_retries: 0
          python_wheel_task:
            package_name: template
            entry_point: main
            parameters: ["--task={{task.name}}",
                         "--env=${bundle.target}",
                        "${var.debug}"]
          libraries:
            - whl: ../dist/*.whl


      job_clusters:
        - job_cluster_key: cluster-dev
          new_cluster:
            spark_version: 15.3.x-scala2.12
            node_type_id: Standard_D8as_v5
            num_workers: 2
            azure_attributes:
              first_on_demand: 1
              availability: SPOT_AZURE
            data_security_mode: SINGLE_USER