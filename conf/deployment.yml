
environments:
  dev:
    strict_path_adjustment_policy: true
    workflows:
      - name: "wf_template_dev"
        job_clusters:
          - job_cluster_key: "cluster_dev"
            new_cluster:
              cluster_source: "JOB"
              spark_version: "14.2.x-scala2.12"
              node_type_id: "c5d.xlarge"
              policy_id: "0016D1880B6C0182"
              num_workers: 1
              aws_attributes:
                first_on_demand: 1
                availability: "SPOT_WITH_FALLBACK"
                zone_id: "auto"
                spot_bid_price_percent: 100
                ebs_volume_count: 0     

        tasks:
          
          - task_key: "task1"
            job_cluster_key: "cluster_dev"
            max_retries: 0
            spark_python_task:
              python_file: "file://tasks/main.py"
              parameters: ["--task=task1", "--env=dev","--input=test"]            

          - task_key: "task2"
            job_cluster_key: "cluster_dev"
            max_retries: 0
            spark_python_task:
              python_file: "file://tasks/main.py"
              parameters: ["--task=task2", "--env=dev","--input=test"]
            depends_on:
            - task_key: "task1"

  prod:
    strict_path_adjustment_policy: true
    workflows:
      - name: "wf_template_prod"
        job_clusters:
          - job_cluster_key: "cluster_prod"
            new_cluster:
              cluster_source: "JOB"
              spark_version: "14.2.x-scala2.12"
              node_type_id: "c5d.xlarge"
              policy_id: "0016D1880B6C0182"
              num_workers: 1
              aws_attributes:
                first_on_demand: 1
                availability: "SPOT_WITH_FALLBACK"
                zone_id: "auto"
                spot_bid_price_percent: 100
                ebs_volume_count: 0     
        tasks:
          
          - task_key: "task1"
            job_cluster_key: "cluster_prod"
            max_retries: 0
            spark_python_task:
              python_file: "file://tasks/main.py"
              parameters: ["--task=task1", "--env=prod","--input=202309-1"]

          - task_key: "task2"
            job_cluster_key: "cluster_prod"
            max_retries: 0
            spark_python_task:
              python_file: "file://tasks/main.py"
              parameters: ["--task=task2", "--env=prod","--input=202309-1"]
            depends_on:
            - task_key: "task1"